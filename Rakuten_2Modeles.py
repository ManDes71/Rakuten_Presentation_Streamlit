# -*- coding: utf-8 -*-
"""
Created on Tue Oct 24 22:45:04 2023

@author: Manuel desplanches
"""
import numpy as np
import streamlit as st
import gc
from datetime import datetime
import random
import time
from  src import Bibli_DataScience_3 as ds
from  src import ML_DataScience as ml
from  src import CNN_DataScience_2 as cnn
from  src import RNN_DataScience as rnn
from sklearn.metrics import f1_score,accuracy_score,confusion_matrix
import matplotlib.pyplot as plt
import itertools
#import gc

import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

from PIL import Image

from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Dense, Input, Activation, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, BatchNormalization, Concatenate, Flatten
from tensorflow.keras import regularizers



class RESEAU_NEURONNES_CAT:
    def __init__(self, reseau,nom_reseau):
        self.__reseau = reseau
        self.__nom_reseau = nom_reseau
        self.__model = None
        self.__model_cat = None
        
        
    def prediction(self,objet_a_predire):
        self.__model = self.__reseau.create_modele()
        self.__model_cat = Model(inputs=self.__model.input, outputs=self.__model.layers[-2].output)
        print(self.__nom_reseau+"_weight_cat")
        ds.load_model(self.__model_cat,self.__nom_reseau+"_weight_cat")
        predictions = self.__model_cat.predict(objet_a_predire)
        #pred = np.argmax(predictions, axis=1)
        return predictions

def st_show_confusion_matrix(y_orig, y_pred):

    
    cnf_matrix = confusion_matrix(y_orig, y_pred,labels=sorted(list(set(y_orig))))
    
    #classes = [10,2280,2403,2705,40,50,2462,1280,1281]
    classes=sorted(list(set(y_orig)))
    b=list(set(y_orig))

    fig2 = plt.figure(figsize=(15,15))

    plt.imshow(cnf_matrix, interpolation='nearest',cmap='Blues')
    plt.title("Matrice de confusion")
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes,rotation=90)
    plt.yticks(tick_marks, classes)

    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):
        plt.text(j, i, cnf_matrix[i, j],
                 horizontalalignment = "center",
                 color = "white" if cnf_matrix[i, j] > ( cnf_matrix.max() / 2) else "black")

    plt.ylabel('Vrais labels')
    plt.xlabel('Labels pr√©dits')
    st.pyplot(fig2)  

def load_image(img):
    im = Image.open(img)
    image = np.array(im)
    return image        
 

def show():
    
    st.write("### Mod√®le concat√©nation LM er RNN")
    st.markdown("""
    <div style='background-color: #f9f9f9; padding: 15px; border-radius: 10px'>
        <h2 style='color: #333;'>üéØ Objectif</h2>
        <p style='font-size: 15px;'>La page pr√©sente un mod√®le qui combine un r√©seau de neurones r√©current (RNN) de type <b>GRU</b> avec un mod√®le de machine learning <b>LinearSVC</b>, en utilisant la fonction de <b>concat√©nation</b> de Keras. Le mod√®le concat√®ne les r√©sultats des deux approches pour produire une pr√©diction finale sur plusieurs classes.</p>
    </div>
    """, unsafe_allow_html=True)

    # Mod√®les utilis√©s
    st.markdown("""
    <div style='background-color: #e8f4fc; padding: 15px; border-radius: 10px'>
        <h2 style='color: #007acc;'>üß† Mod√®les utilis√©s</h2>
        <ul style='font-size: 15px;'>
            <li><b>RNN (GRU)</b> : Ce mod√®le traite les s√©quences textuelles et utilise les couches GRU pour effectuer des pr√©dictions. Les r√©sultats sont √©valu√©s avec des m√©triques telles que le <b>F1 Score</b> et l'<b>accuracy</b>.</li>
            <li><b>LinearSVC</b> : Ce mod√®le de machine learning traite √©galement des donn√©es textuelles et est √©valu√© de mani√®re similaire avec des <b>scores F1</b> et <b>accuracy</b>.</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)

    # Combinaison des mod√®les
    st.markdown("""
    <div style='background-color: #f4f4e8; padding: 15px; border-radius: 10px'>
        <h2 style='color: #444;'>üîó Combinaison des mod√®les</h2>
        <p style='font-size: 15px;'>Les deux mod√®les, <b>GRU</b> et <b>LinearSVC</b>, sont concat√©n√©s juste avant la couche de pr√©diction, permettant de combiner les avantages des r√©seaux de neurones r√©currents et des mod√®les de machine learning lin√©aires.</p>
    </div>
    """, unsafe_allow_html=True)

    # √âvaluation du mod√®le
    st.markdown("""
    <div style='background-color: #f9e9e9; padding: 15px; border-radius: 10px'>
        <h2 style='color: #d9534f;'>üìä √âvaluation du mod√®le</h2>
        <p style='font-size: 15px;'>Le mod√®le final est compil√© avec l'optimiseur <b>Adam</b> et la fonction de perte <b>categorical_crossentropy</b>. 
        Un entra√Ænement est r√©alis√© sur plusieurs √©poques, avec un suivi des performances (perte, pr√©cision) via une barre de progression interactive.</p>
        <p style='font-size: 15px;'>Les performances finales sont √©valu√©es en termes de <b>loss</b>, <b>accuracy</b>, et <b>F1 Score</b>. Les r√©sultats sont √©galement affich√©s via une <b>matrice de confusion</b> pour visualiser les pr√©dictions du mod√®le.</p>
    </div>
    """, unsafe_allow_html=True)

    # Pr√©diction finale
    st.markdown("""
    <div style='background-color: #f9f9f9; padding: 15px; border-radius: 10px'>
        <h2 style='color: #333;'>üîç Pr√©diction finale</h2>
        <p style='font-size: 15px;'>Une pr√©diction est effectu√©e sur l'ensemble de test en combinant les donn√©es des deux mod√®les (<b>GRU</b> et <b>LinearSVC</b>). La <b>classe pr√©dite</b> est ensuite affich√©e, et des m√©triques suppl√©mentaires comme l'<b>accuracy</b> et le <b>F1 Score</b> sont calcul√©es pour √©valuer la qualit√© des pr√©dictions.</p>
    </div>
    """, unsafe_allow_html=True)


        
    st.markdown("""**rappel des mod√®les** : """)      
   
    st.write("Mod√®le RNN EMBEDDING GRU") 
    gru = rnn.RNN_GRU("EMBEDDING GRU")
    y_orig,y_pred = gru.restore_predict_arrays()
    f1 = f1_score(y_orig, y_pred, average='weighted')
    acc_score,classif=ds.get_classification_report(y_orig, y_pred)
    st.write("Accuracy: ", acc_score/100)
    st.write("F1 Score: ", f1)

    del y_orig, y_pred, f1, acc_score,classif
    gru = None
    
    train_X_gru = ds.load_ndarray('EMBEDDING GRU_CONCAT2_X_train') 
    test_X_gru = ds.load_ndarray('EMBEDDING_GRU_CONCAT2_X_test') 
    #train_y_gru = ds.load_ndarray('EMBEDDING GRU_y_train')
    #test_y_gru = ds.load_ndarray('EMBEDDING GRU_y_test')
   
  
    st.write("Mod√®le LinearSVC") 
    
    
    #lsvc =  ml.ML_LinearSVCFromModel("LinearSVCFromModel",process=False)
    #lsvc_mod=lsvc.load_modele()
    
    
    #train_X_svc = ds.load_ndarray('LinearSVCFromModel_CONCAT2_X_train') 
    #test_X_svc = ds.load_ndarray('LinearSVCFromModel_CONCAT2_X_test') 
    #train_y_svc = ds.load_ndarray('LinearSVCFromModel_CONCAT2_y_train') 
    #test_y_svc = ds.load_ndarray('LinearSVCFromModel_CONCAT2_y_test') 

   
    #lsvc =  ml.ML_LinearSVCFromModel("LinearSVC",process=False)
    #lsvc_mod=lsvc.load_modele()
    
    
    train_X_svc = ds.load_ndarray('LinearSVC_CONCAT2_X_train') 
    test_X_svc = ds.load_ndarray('LinearSVC_CONCAT2_X_test') 
    train_y_svc = ds.load_ndarray('LinearSVC_CONCAT2_y_train')
    test_y_svc = ds.load_ndarray('LinearSVC_CONCAT2_y_test')

    lsvc = None
    gc.collect()
    lr = ml.ML_LinearSVC("LinearSVC",process=False)

    lr_mod = lr.load_modele()
    y_orig = lr.get_y_orig()
    y_pred = lr.get_y_pred()

    f1 = f1_score(y_orig.values, y_pred, average='weighted')
    acc_score,classif=ds.get_classification_report(y_orig.values, y_pred)
    st.write("Accuracy: ", acc_score/100)
    st.write("F1 Score: ", f1)

    del y_orig, y_pred, f1, acc_score, classif
    lr_mod = None


    st.markdown("""**Entrainement du mod√®le commun** (agr√©gation des 2 mod√®les par la fonction **Concatenate** de TensorFlow)  : environ 20 s """)

    print("train_X_svc.shape = ",train_X_svc.shape)
    print("train_X_gru.shape = ",train_X_gru.shape)


    [nSamp,inpShape_svc] = train_X_svc.shape
    print("nSamp = ",nSamp)
    [nSamp,inpShape_gru] = train_X_gru.shape
    print("nSamp = ",nSamp)
    
    print("inpShape_svc = ",inpShape_svc)
    print("inpShape_gru = ",inpShape_gru)
   
    
    from sklearn.preprocessing import LabelEncoder
    from tensorflow.keras.utils import to_categorical
    label_encoder = LabelEncoder()
    
    
    y_classes_converted = label_encoder.fit_transform(train_y_svc)
    y_train_Network = to_categorical(y_classes_converted)
    y_classes_converted = label_encoder.transform(test_y_svc)
    y_test_Network = to_categorical(y_classes_converted)

    del train_y_svc
    gc.collect()

    num_classes=27


    seq_modelsvc = Sequential()     #   svc
    seq_modelsvc.add(Dense(128, activation='relu',input_shape = (inpShape_svc,), name = "Input1"))
    seq_modelsvc.add(Dense(64, activation='relu'))
    seq_modelsvc.add(Dense(27,activation='softmax'))  # 27 neurones pour 27 classes
    
    
    
    
    seq_modelgru = Sequential()    #  gru
    seq_modelgru.add(Dense(128, activation='relu',input_shape = (inpShape_gru,), name = "Input2"))
    seq_modelgru.add(Dense(64, activation='relu'))
    seq_modelgru.add(Dense(27,activation='softmax'))  # 27 neurones pour 27 classes
    del y_classes_converted
    
    # D√©finir les couches d'entr√©e explicitement
    input_svc = Input(shape=(inpShape_svc,))
    input_gru = Input(shape=(inpShape_gru,))

    # Appeler les mod√®les avec les donn√©es d'entr√©e correspondantes
    svc_output = seq_modelsvc(input_svc)  # Utiliser train_X_svc comme entr√©e pour seq_modelsvc
    del input_svc
    gc.collect()
    gru_output = seq_modelgru(input_gru)  # Utiliser train_X_gru comme entr√©e pour seq_modelgru
    del input_gru
    gc.collect()
    # Concat√©ner les deux mod√®les
    concat_layer = Concatenate()([svc_output, gru_output])
    del svc_output, gru_output
    gc.collect()
    
    normalized_layer = BatchNormalization()(concat_layer)
    # Ajoutez des couches suppl√©mentaires si n√©cessaire
    final_output = Dense(num_classes, activation='softmax')(normalized_layer)
    
    # Cr√©er le mod√®le final
    final_model = Model(inputs=[seq_modelsvc.input, seq_modelgru.input], outputs=final_output)
    
    # R√©sum√© du mod√®le
    final_model.summary()

    seq_modelsvc = None
    seq_modelgru = None
    #del train_X_svc, train_X_gru
    #gc.collect()  # Lib√©ration manuelle de la m√©moire

    print("avant compile")
    
    # Compilation du mod√®le
    final_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    print("apres compile")
    #print(train_X_svc.shape)
    #print(train_X_gru.shape)
    


    print("Entra√Ænement du mod√®le...")

    progress_bar = st.progress(0)
    status_text = st.empty()

   

    #st.write("Entra√Ænement du mod√®le...")
    
    class ProgressCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            progress_bar.progress((epoch + 1) / 3)  # 10 √©poques dans votre cas
            status_text.text(f"√âpoque actuelle : {epoch + 1}, Perte : {logs['loss']:.4f}, Pr√©cision : {logs['accuracy']:.4f}")

    # Entra√Ænement du mod√®le
    with tf.device('/CPU:0'):
        final_model.fit([train_X_svc,train_X_gru], y=y_train_Network, epochs=3, batch_size=32,
                       callbacks=[ProgressCallback()])

    print("mod√®le entrainn√©")
    #ds.save_model(final_model,"Rakuten_2M_weight.weights") 
    #ds.save_ndarray(label_encoder,"Rakuten_2M_label_encoder")
    
    #ds.load_model(final_model,"Rakuten_2M_weight.weights")
    #print("modele charg√©")
    #label_encoder = ds.load_ndarray("Rakuten_2M_label_encoder")
    #√âvaluation du mod√®le
    st.write("Appliquons le mod√®le sur le jeu de test :")
    loss, accuracy = final_model.evaluate([ test_X_svc,test_X_gru], y=y_test_Network)
    st.write(f'Loss: {loss}, Accuracy: {accuracy}')
        
   
    
    #ds.save_model(final_model,"Rakuten_2M_weight") 
    #ds.save_ndarray(label_encoder,"Rakuten_2M_label_encoder")
    
    prediction = final_model.predict([ test_X_svc,test_X_gru])
    #print("prediction : ",prediction)
    predicted_class = np.argmax(prediction, axis=1)
    y_pred = label_encoder.inverse_transform(predicted_class)

    #print(f"La classe pr√©dite est : {predicted_class}")
    y_orig=test_y_svc
    accuracy = accuracy_score(y_orig,y_pred)
    f1 = f1_score(y_orig, y_pred,average='weighted')
    
    # Affichage des r√©sultats
    st.markdown("""**Performance du mod√®le** :""")
    st.write(f"Accuracy: {accuracy:.4f}")
    st.write(f"F1 Score: {f1:.4f}")
    acc_score,classif=ds.get_classification_report(y_orig, y_pred)
 
    st.write("Accuracy: ", acc_score/100)
    st.write("F1 Score: ", f1)
    st.markdown("""**Rapport de classification** : """)
    st.markdown(f"```\n{classif}\n```")
    st.markdown("""**Matrice de confusion** : """)
    st_show_confusion_matrix(y_orig, y_pred)
    del  test_X_svc,  test_X_gru
    del y_orig, y_pred
    del acc_score, classif,f1
    del  y_train_Network, y_test_Network
    del loss, accuracy, prediction, predicted_class
    final_model = None
    gc.collect()  # Lib√©ration manuelle de la m√©moire

    

